{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertTokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "class_ind = {'CC':0, 'EC':1, 'LGSC':2, 'HGSC':3, 'MC':4}\n",
    "\n",
    "class OvarianImages(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir):\n",
    "        self.img_metadata = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = idx % len(self.img_metadata) \n",
    "        sample = self.img_metadata.iloc[sample_idx, 0]\n",
    "\n",
    "        label = torch.tensor(class_ind[self.img_metadata.iloc[sample_idx, 1]]).to(device)  \n",
    "\n",
    "        image_patches = []\n",
    "        for i in range(100):\n",
    "            img_path = self.img_dir + f'sample_{sample}' + f'/{sample}_{i}.png'\n",
    "            patch = torch.tensor(np.asarray(Image.open(img_path))[13:237, 13:237].T, dtype=torch.float32)\n",
    "            image_patches.append(patch)\n",
    "        image_patches = torch.stack(image_patches, dim=0).to(device)\n",
    "        return image_patches, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "metadata = \"/scratch1/yuqiuwan/CSCI567/train.csv\"\n",
    "image_dir = \"/scratch1/yuqiuwan/CSCI567/preprocess_images/\"\n",
    "imgs = OvarianImages(metadata, image_dir)\n",
    "train_set, test_set = torch.utils.data.random_split(imgs, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Base model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, FeatureExtractor, d_embed=768, n_classes=5):\n",
    "        super().__init__()\n",
    "        self.image_encoder = FeatureExtractor\n",
    "        self.image_proj = nn.Linear(d_embed, n_classes)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        outputs = self.image_encoder(*args, **kwargs)\n",
    "        features = outputs.last_hidden_state[:, 0, :] \n",
    "\n",
    "        logits = self.image_proj(features).view(-1, 100, self.n_classes)\n",
    "        logits = torch.sum(logits, axis=1)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at owkin/phikon were not used when initializing ViTModel: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, image_processor=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (imgs, labels) in enumerate(dataloader): \n",
    "        imgs = imgs.view(-1, 3, 224, 224)\n",
    "        if image_processor:\n",
    "            imgs = image_processor(imgs, return_tensors=\"pt\").to(device)\n",
    "            # Compute prediction and loss\n",
    "            logits = model(imgs['pixel_values'])\n",
    "        else:\n",
    "            # Compute prediction and loss\n",
    "            logits = model(imgs)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc = torch.sum(torch.argmax(logits, axis=1) == labels) / logits.shape[0]\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * batch_size\n",
    "            print(f\"loss: {loss:>7f}; train_acc: {train_acc:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "######################## Train Base Model ########################\n",
    "batch_size = 1\n",
    "dropout = 0.0\n",
    "learning_rate = 10**(-3)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# load phikon\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"owkin/phikon\")\n",
    "extractor = ViTModel.from_pretrained(\"owkin/phikon\", add_pooling_layer=False)\n",
    "extractor.eval()\n",
    "\n",
    "# Initialize base model\n",
    "base_model = BaselineModel(extractor, d_embed=768, n_classes=5).to(device)\n",
    "# Freeze the pre-trained model\n",
    "for p in base_model.image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(base_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, base_model, F.cross_entropy, optimizer, image_processor=image_processor)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(base_model.state_dict(), '/scratch1/yuqiuwan/CSCI567/phikon_model_state_dict.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"owkin/phikon\")\n",
    "extractor = ViTModel.from_pretrained(\"owkin/phikon\", add_pooling_layer=False)\n",
    "extractor.eval()\n",
    "\n",
    "# Initialize base model\n",
    "base_model = BaselineModel(extractor, d_embed=768, n_classes=5)\n",
    "base_model.load_state_dict(torch.load('/scratch1/yuqiuwan/CSCI567/phikon_model_state_dict2.pt'))\n",
    "base_model.eval()\n",
    "base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 correct_num: tensor(1, device='cuda:0')\n",
      "1 correct_num: tensor(2, device='cuda:0')\n",
      "2 correct_num: tensor(3, device='cuda:0')\n",
      "3 correct_num: tensor(4, device='cuda:0')\n",
      "4 correct_num: tensor(5, device='cuda:0')\n",
      "5 correct_num: tensor(6, device='cuda:0')\n",
      "6 correct_num: tensor(7, device='cuda:0')\n",
      "7 correct_num: tensor(8, device='cuda:0')\n",
      "8 correct_num: tensor(9, device='cuda:0')\n",
      "9 correct_num: tensor(10, device='cuda:0')\n",
      "10 correct_num: tensor(11, device='cuda:0')\n",
      "11 correct_num: tensor(12, device='cuda:0')\n",
      "12 correct_num: tensor(13, device='cuda:0')\n",
      "13 correct_num: tensor(13, device='cuda:0')\n",
      "14 correct_num: tensor(14, device='cuda:0')\n",
      "15 correct_num: tensor(15, device='cuda:0')\n",
      "16 correct_num: tensor(16, device='cuda:0')\n",
      "17 correct_num: tensor(17, device='cuda:0')\n",
      "18 correct_num: tensor(18, device='cuda:0')\n",
      "19 correct_num: tensor(19, device='cuda:0')\n",
      "20 correct_num: tensor(20, device='cuda:0')\n",
      "21 correct_num: tensor(21, device='cuda:0')\n",
      "22 correct_num: tensor(22, device='cuda:0')\n",
      "23 correct_num: tensor(23, device='cuda:0')\n",
      "24 correct_num: tensor(24, device='cuda:0')\n",
      "25 correct_num: tensor(25, device='cuda:0')\n",
      "26 correct_num: tensor(26, device='cuda:0')\n",
      "27 correct_num: tensor(27, device='cuda:0')\n",
      "28 correct_num: tensor(27, device='cuda:0')\n",
      "29 correct_num: tensor(27, device='cuda:0')\n",
      "30 correct_num: tensor(28, device='cuda:0')\n",
      "31 correct_num: tensor(29, device='cuda:0')\n",
      "32 correct_num: tensor(30, device='cuda:0')\n",
      "33 correct_num: tensor(31, device='cuda:0')\n",
      "34 correct_num: tensor(31, device='cuda:0')\n",
      "35 correct_num: tensor(32, device='cuda:0')\n",
      "36 correct_num: tensor(33, device='cuda:0')\n",
      "37 correct_num: tensor(34, device='cuda:0')\n",
      "38 correct_num: tensor(35, device='cuda:0')\n",
      "39 correct_num: tensor(35, device='cuda:0')\n",
      "40 correct_num: tensor(36, device='cuda:0')\n",
      "41 correct_num: tensor(37, device='cuda:0')\n",
      "42 correct_num: tensor(38, device='cuda:0')\n",
      "43 correct_num: tensor(39, device='cuda:0')\n",
      "44 correct_num: tensor(40, device='cuda:0')\n",
      "45 correct_num: tensor(40, device='cuda:0')\n",
      "46 correct_num: tensor(41, device='cuda:0')\n",
      "47 correct_num: tensor(42, device='cuda:0')\n",
      "48 correct_num: tensor(43, device='cuda:0')\n",
      "49 correct_num: tensor(44, device='cuda:0')\n",
      "50 correct_num: tensor(45, device='cuda:0')\n",
      "51 correct_num: tensor(45, device='cuda:0')\n",
      "52 correct_num: tensor(46, device='cuda:0')\n",
      "53 correct_num: tensor(47, device='cuda:0')\n",
      "54 correct_num: tensor(48, device='cuda:0')\n",
      "55 correct_num: tensor(48, device='cuda:0')\n",
      "56 correct_num: tensor(49, device='cuda:0')\n",
      "57 correct_num: tensor(50, device='cuda:0')\n",
      "58 correct_num: tensor(50, device='cuda:0')\n",
      "59 correct_num: tensor(51, device='cuda:0')\n",
      "60 correct_num: tensor(51, device='cuda:0')\n",
      "61 correct_num: tensor(52, device='cuda:0')\n",
      "62 correct_num: tensor(52, device='cuda:0')\n",
      "63 correct_num: tensor(52, device='cuda:0')\n",
      "64 correct_num: tensor(53, device='cuda:0')\n",
      "65 correct_num: tensor(53, device='cuda:0')\n",
      "66 correct_num: tensor(54, device='cuda:0')\n",
      "67 correct_num: tensor(54, device='cuda:0')\n",
      "68 correct_num: tensor(55, device='cuda:0')\n",
      "69 correct_num: tensor(56, device='cuda:0')\n",
      "70 correct_num: tensor(57, device='cuda:0')\n",
      "71 correct_num: tensor(58, device='cuda:0')\n",
      "72 correct_num: tensor(59, device='cuda:0')\n",
      "73 correct_num: tensor(59, device='cuda:0')\n",
      "74 correct_num: tensor(60, device='cuda:0')\n",
      "75 correct_num: tensor(60, device='cuda:0')\n",
      "76 correct_num: tensor(60, device='cuda:0')\n",
      "77 correct_num: tensor(61, device='cuda:0')\n",
      "78 correct_num: tensor(62, device='cuda:0')\n",
      "79 correct_num: tensor(63, device='cuda:0')\n",
      "80 correct_num: tensor(64, device='cuda:0')\n",
      "81 correct_num: tensor(65, device='cuda:0')\n",
      "82 correct_num: tensor(66, device='cuda:0')\n",
      "83 correct_num: tensor(67, device='cuda:0')\n",
      "84 correct_num: tensor(68, device='cuda:0')\n",
      "85 correct_num: tensor(69, device='cuda:0')\n",
      "86 correct_num: tensor(70, device='cuda:0')\n",
      "87 correct_num: tensor(71, device='cuda:0')\n",
      "88 correct_num: tensor(72, device='cuda:0')\n",
      "89 correct_num: tensor(72, device='cuda:0')\n",
      "90 correct_num: tensor(73, device='cuda:0')\n",
      "91 correct_num: tensor(74, device='cuda:0')\n",
      "92 correct_num: tensor(75, device='cuda:0')\n",
      "93 correct_num: tensor(76, device='cuda:0')\n",
      "94 correct_num: tensor(77, device='cuda:0')\n",
      "95 correct_num: tensor(78, device='cuda:0')\n",
      "96 correct_num: tensor(79, device='cuda:0')\n",
      "97 correct_num: tensor(80, device='cuda:0')\n",
      "98 correct_num: tensor(81, device='cuda:0')\n",
      "99 correct_num: tensor(82, device='cuda:0')\n",
      "100 correct_num: tensor(82, device='cuda:0')\n",
      "101 correct_num: tensor(83, device='cuda:0')\n",
      "102 correct_num: tensor(83, device='cuda:0')\n",
      "103 correct_num: tensor(84, device='cuda:0')\n",
      "104 correct_num: tensor(85, device='cuda:0')\n",
      "105 correct_num: tensor(85, device='cuda:0')\n",
      "106 correct_num: tensor(86, device='cuda:0')\n",
      "Test_accuracy: tensor(0.8037, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "\n",
    "def test_loop(dataloader, model, image_processor=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    correct_num = 0\n",
    "    for batch, (imgs, labels) in enumerate(dataloader): \n",
    "        imgs = imgs.view(-1, 3, 224, 224)\n",
    "        if image_processor:\n",
    "            imgs = image_processor(imgs, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**imgs)\n",
    "        else:\n",
    "            logits = model(imgs)\n",
    "\n",
    "        correct_num += torch.sum(torch.argmax(logits, axis=1) == labels) \n",
    "        print(batch, 'correct_num:', correct_num)\n",
    "    \n",
    "    test_acc = correct_num  / size\n",
    "    print('Test_accuracy:', test_acc)\n",
    "\n",
    "        \n",
    "test_loop(test_dataloader, base_model, image_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
