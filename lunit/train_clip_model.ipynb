{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load image data and text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import BertModel, BertTokenizer\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "torch.manual_seed(13)\n",
    "\n",
    "class_ind = {'CC':0, 'EC':1, 'LGSC':2, 'HGSC':3, 'MC':4}\n",
    "\n",
    "class OvarianDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, text_dir):\n",
    "        self.img_metadata = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.texts = {}\n",
    "        for c in ['CC', 'EC', 'LGSC', 'HGSC', 'MC']:\n",
    "            self.texts[c] = pd.read_table(text_dir+c+'.txt', header=None, sep='.').iloc[:,1].to_list()\n",
    "       \n",
    "    def __len__(self):\n",
    "        return len(self.img_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_idx = idx\n",
    "        sample = self.img_metadata.iloc[sample_idx, 0]\n",
    "        group = self.img_metadata.iloc[sample_idx, 1]\n",
    "\n",
    "        label = torch.tensor(class_ind[group]).to(device)  \n",
    "\n",
    "        image_patches = []\n",
    "        for i in range(100):\n",
    "            img_path = self.img_dir + f'sample_{sample}' + f'/{sample}_{i}.png'\n",
    "            patch = torch.tensor(np.asarray(Image.open(img_path))[13:237, 13:237].T, dtype=torch.float32)\n",
    "            image_patches.append(patch)\n",
    "        image_patches = torch.stack(image_patches, dim=0).to(device)\n",
    "\n",
    "        text = self.texts[group][idx % 100]\n",
    "        # text = torch.tensor(self.tokenizer.encode(text, max_length=seq_max_length, padding=\"max_length\")).to(device)\n",
    "\n",
    "        return image_patches, text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset\n",
    "metadata = \"/scratch1/yuqiuwan/CSCI567/train.csv\"\n",
    "image_dir = \"/scratch1/yuqiuwan/CSCI567/preprocess_images_threshold/\"\n",
    "text_dir = \"/scratch1/yuqiuwan/CSCI567/textLabel/\"\n",
    "\n",
    "wholedataset = OvarianDataset(metadata, image_dir, text_dir)\n",
    "train_set, test_set = torch.utils.data.random_split(wholedataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, ImageEncoder, TextEncoder, d_embed=[384,768], n_classes=5):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder\n",
    "        self.text_encoder = TextEncoder\n",
    "        self.image_proj = nn.Linear(d_embed[0], n_classes)\n",
    "        self.text_proj = nn.Linear(d_embed[1], n_classes)\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, img, text):\n",
    "        img_features = self.image_encoder(img)\n",
    "        img_embed = self.image_proj(img_features).view(-1, 100, self.n_classes)\n",
    "        img_embed = torch.mean(img_embed, 1)\n",
    "        img_embed = img_embed / torch.norm(img_embed, dim=-1, keepdim=True)\n",
    "\n",
    "        text_outputs = self.text_encoder(**text)\n",
    "        text_embed = text_outputs.pooler_output\n",
    "        text_embed = self.text_proj(text_embed)\n",
    "        text_embed = text_embed / torch.norm(text_embed, dim=-1, keepdim=True)\n",
    "\n",
    "        logits = img_embed @ text_embed.T\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "def get_pretrained_url(key):\n",
    "    URL_PREFIX = \"https://github.com/lunit-io/benchmark-ssl-pathology/releases/download/pretrained-weights\"\n",
    "    model_zoo_registry = {\n",
    "        \"DINO_p16\": \"dino_vit_small_patch16_ep200.torch\",\n",
    "        \"DINO_p8\": \"dino_vit_small_patch8_ep200.torch\",\n",
    "    }\n",
    "    pretrained_url = f\"{URL_PREFIX}/{model_zoo_registry.get(key)}\"\n",
    "    return pretrained_url\n",
    "\n",
    "\n",
    "def vit_small(pretrained, progress, key, **kwargs):\n",
    "    patch_size = kwargs.get(\"patch_size\", 16)\n",
    "    model = VisionTransformer(\n",
    "        img_size=224, patch_size=patch_size, embed_dim=384, num_heads=6, num_classes=0\n",
    "    )\n",
    "    if pretrained:\n",
    "        pretrained_url = get_pretrained_url(key)\n",
    "        verbose = model.load_state_dict(\n",
    "            torch.hub.load_state_dict_from_url(pretrained_url, progress=progress)\n",
    "        )\n",
    "        print(verbose)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "seq_max_length = 50\n",
    "tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "def contrastive_loss(logits, labels):\n",
    "    image_loss = F.cross_entropy(logits, labels, reduction=\"mean\")\n",
    "    text_loss = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n",
    "    loss = (image_loss + text_loss) / 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, image_processor=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    best_loss = np.inf\n",
    "    for batch, (imgs, texts, _) in enumerate(dataloader):\n",
    "        labels = torch.tensor(range(batch_size)).to(device)\n",
    "        imgs = imgs.view(-1, 3, 224, 224)\n",
    "        texts = tokenizer(texts, padding='max_length', max_length=seq_max_length, return_tensors='pt').to(device)\n",
    "\n",
    "        if image_processor:\n",
    "            imgs = image_processor(imgs, return_tensors=\"pt\").to(device)\n",
    "            # Compute prediction and loss\n",
    "            logits = model(imgs['pixel_values'], texts)\n",
    "        else:\n",
    "            # Compute prediction and loss\n",
    "            logits = model(imgs, texts)\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_acc = torch.sum(torch.argmax(logits, axis=1) == labels) / logits.shape[0]\n",
    "\n",
    "        loss, current = loss.item(), (batch + 1) * batch_size\n",
    "        if train_acc == 1:\n",
    "            if loss < best_loss:\n",
    "                torch.save(model.state_dict(), '/scratch1/yuqiuwan/CSCI567/bert_lunit_model_state_dict_current_best.pt')\n",
    "            \n",
    "      \n",
    "        print(f\"loss: {loss:>7f}; train_acc: {train_acc:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "######################## Train CLIP ########################\n",
    "batch_size = 2\n",
    "dropout = 0.0\n",
    "learning_rate = 10**(-3)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# load phikon\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"owkin/phikon\")\n",
    "img_encoder = vit_small(pretrained=True, progress=False, key=\"DINO_p16\", patch_size=16)\n",
    "img_encoder.eval()\n",
    "\n",
    "# load bert\n",
    "text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "text_encoder.eval()\n",
    "\n",
    "clip_model = CLIP(img_encoder, text_encoder, d_embed=[384, 768], n_classes=5).to(device)\n",
    "\n",
    "# Freeze the pre-trained model\n",
    "for p in clip_model.image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in clip_model.text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "optimizer = torch.optim.Adam(clip_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, clip_model, contrastive_loss, optimizer, image_processor)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/scratch1/yuqiuwan/CSCI567/bert_lunit_model_state_dict_last_epoch_output.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"owkin/phikon\")\n",
    "img_encoder = vit_small(pretrained=True, progress=False, key=\"DINO_p16\", patch_size=16)\n",
    "img_encoder.eval()\n",
    "# load bert\n",
    "text_encoder = BertModel.from_pretrained('bert-base-uncased')\n",
    "text_encoder.eval()\n",
    "\n",
    "clip_model = CLIP(img_encoder, text_encoder, d_embed=[384, 768], n_classes=5).to(device)\n",
    "clip_model.load_state_dict(torch.load('/scratch1/yuqiuwan/CSCI567/bert_lunit_model_state_dict_last_epoch_output.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4119, -0.7330, -0.6756,  0.5110, -0.8785]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "0 correct_num: tensor(1, device='cuda:0')\n",
      "tensor([[ 0.9085, -0.3696,  0.3149, -0.0672,  0.3685]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "1 correct_num: tensor(2, device='cuda:0')\n",
      "tensor([[-0.9527, -0.2206, -0.5433,  0.4799, -0.7716]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "2 correct_num: tensor(3, device='cuda:0')\n",
      "tensor([[-0.8024,  0.0017, -0.0387,  0.4935, -0.2982]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "3 correct_num: tensor(4, device='cuda:0')\n",
      "tensor([[-0.6433,  0.7872, -0.2268, -0.4326, -0.0730]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "4 correct_num: tensor(5, device='cuda:0')\n",
      "tensor([[-0.7546,  0.7108, -0.4456, -0.4166, -0.2887]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([2], device='cuda:0')\n",
      "5 correct_num: tensor(5, device='cuda:0')\n",
      "tensor([[-0.6269,  0.8169, -0.3927, -0.5659, -0.1730]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "6 correct_num: tensor(6, device='cuda:0')\n",
      "tensor([[-0.7045, -0.6392, -0.3726,  0.7821, -0.7361]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "7 correct_num: tensor(7, device='cuda:0')\n",
      "tensor([[ 0.9188,  0.2484,  0.3909, -0.5710,  0.6677]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "8 correct_num: tensor(8, device='cuda:0')\n",
      "tensor([[-0.7559, -0.5586, -0.3199,  0.7841, -0.6809]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "9 correct_num: tensor(9, device='cuda:0')\n",
      "tensor([[-0.6266, -0.2869, -0.5922,  0.2390, -0.7182]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "10 correct_num: tensor(10, device='cuda:0')\n",
      "tensor([[-0.6798,  0.7444, -0.5567, -0.5588, -0.3246]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "11 correct_num: tensor(11, device='cuda:0')\n",
      "tensor([[-0.6913,  0.6372, -0.2107, -0.2648, -0.1090]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "12 correct_num: tensor(12, device='cuda:0')\n",
      "tensor([[ 0.4054, -0.8578,  0.5029,  0.8048,  0.1568]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "13 correct_num: tensor(13, device='cuda:0')\n",
      "tensor([[-0.6645,  0.0460, -0.1750,  0.3121, -0.2566]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "14 correct_num: tensor(14, device='cuda:0')\n",
      "tensor([[-0.6454, -0.7022, -0.4229,  0.7670, -0.7775]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "15 correct_num: tensor(15, device='cuda:0')\n",
      "tensor([[ 0.5907,  0.6913,  0.1439, -0.9041,  0.5559]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "16 correct_num: tensor(15, device='cuda:0')\n",
      "tensor([[-0.5036, -0.2730,  0.3187,  0.7477, -0.0481]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "17 correct_num: tensor(16, device='cuda:0')\n",
      "tensor([[ 0.8157,  0.0464,  0.0543, -0.5330,  0.3122]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "18 correct_num: tensor(17, device='cuda:0')\n",
      "tensor([[-0.9737,  0.1368, -0.7315,  0.0694, -0.7650]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "19 correct_num: tensor(18, device='cuda:0')\n",
      "tensor([[-0.7618, -0.5740, -0.4894,  0.7113, -0.7981]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "20 correct_num: tensor(19, device='cuda:0')\n",
      "tensor([[-0.7104, -0.6307, -0.3738,  0.7948, -0.7261]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "21 correct_num: tensor(20, device='cuda:0')\n",
      "tensor([[-0.9253, -0.3040, -0.5784,  0.5071, -0.8179]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "22 correct_num: tensor(21, device='cuda:0')\n",
      "tensor([[-0.5032, -0.2545,  0.3218,  0.7288, -0.0434]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "23 correct_num: tensor(21, device='cuda:0')\n",
      "tensor([[ 0.7365, -0.4369, -0.0055, -0.0938,  0.0608]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "24 correct_num: tensor(22, device='cuda:0')\n",
      "tensor([[-0.0693, -0.9702,  0.0489,  0.9354, -0.3560]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "25 correct_num: tensor(23, device='cuda:0')\n",
      "tensor([[ 0.8582, -0.3963,  0.2071, -0.0831,  0.2744]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "26 correct_num: tensor(24, device='cuda:0')\n",
      "tensor([[-0.3971, -0.8809, -0.2941,  0.8509, -0.6727]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "27 correct_num: tensor(25, device='cuda:0')\n",
      "tensor([[ 0.9770, -0.2329,  0.6392, -0.0438,  0.6664]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "28 correct_num: tensor(26, device='cuda:0')\n",
      "tensor([[-0.5443, -0.7919, -0.3346,  0.8481, -0.7102]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "29 correct_num: tensor(27, device='cuda:0')\n",
      "tensor([[-0.6102, -0.7404, -0.4189,  0.7941, -0.7727]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "30 correct_num: tensor(28, device='cuda:0')\n",
      "tensor([[ 0.3391, -0.8359,  0.5402,  0.8420,  0.1681]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "31 correct_num: tensor(29, device='cuda:0')\n",
      "tensor([[ 0.9419,  0.2100,  0.4507, -0.5205,  0.6950]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "32 correct_num: tensor(30, device='cuda:0')\n",
      "tensor([[ 0.3494, -0.3978,  0.8592,  0.6252,  0.5488]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([2], device='cuda:0')\n",
      "33 correct_num: tensor(31, device='cuda:0')\n",
      "tensor([[-0.3834,  0.9310, -0.2885, -0.7632,  0.0154]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "34 correct_num: tensor(32, device='cuda:0')\n",
      "tensor([[-0.0254, -0.6940,  0.5176,  0.9335,  0.0875]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([2], device='cuda:0')\n",
      "35 correct_num: tensor(32, device='cuda:0')\n",
      "tensor([[-0.7580, -0.5825, -0.4031,  0.7568, -0.7462]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "36 correct_num: tensor(33, device='cuda:0')\n",
      "tensor([[-0.2197, -0.9457, -0.1166,  0.9121, -0.5131]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "37 correct_num: tensor(34, device='cuda:0')\n",
      "tensor([[ 0.8871, -0.2745,  0.2331, -0.1982,  0.3354]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "38 correct_num: tensor(35, device='cuda:0')\n",
      "tensor([[ 0.4600, -0.6699,  0.7302,  0.7310,  0.3965]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "39 correct_num: tensor(35, device='cuda:0')\n",
      "tensor([[-0.5880, -0.7519, -0.3061,  0.8590, -0.6875]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "40 correct_num: tensor(36, device='cuda:0')\n",
      "tensor([[-0.4854, -0.8217, -0.2255,  0.8943, -0.6274]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "41 correct_num: tensor(37, device='cuda:0')\n",
      "tensor([[-0.5046,  0.8917, -0.2863, -0.6370, -0.0191]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([4], device='cuda:0')\n",
      "42 correct_num: tensor(37, device='cuda:0')\n",
      "tensor([[-0.8624, -0.2475, -0.2916,  0.5706, -0.5949]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "43 correct_num: tensor(38, device='cuda:0')\n",
      "tensor([[ 0.1208, -0.8850,  0.3820,  0.9445, -0.0188]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "44 correct_num: tensor(39, device='cuda:0')\n",
      "tensor([[ 0.8904, -0.1952,  0.2014, -0.2769,  0.3444]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "45 correct_num: tensor(40, device='cuda:0')\n",
      "tensor([[-0.4913,  0.8275, -0.5134, -0.7386, -0.2045]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "46 correct_num: tensor(41, device='cuda:0')\n",
      "tensor([[ 0.5314,  0.6802,  0.5603, -0.6337,  0.7918]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "47 correct_num: tensor(41, device='cuda:0')\n",
      "tensor([[-0.9744, -0.1358, -0.5505,  0.4065, -0.7516]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "48 correct_num: tensor(41, device='cuda:0')\n",
      "tensor([[-0.8727, -0.0281, -0.9090,  0.0303, -0.9311]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "49 correct_num: tensor(42, device='cuda:0')\n",
      "tensor([[-0.8754,  0.1289, -0.8984, -0.1031, -0.8694]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "50 correct_num: tensor(43, device='cuda:0')\n",
      "tensor([[-0.6793,  0.7784, -0.3531, -0.4740, -0.1751]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "51 correct_num: tensor(44, device='cuda:0')\n",
      "tensor([[-0.8262, -0.2442, -0.1451,  0.6516, -0.4684]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "52 correct_num: tensor(45, device='cuda:0')\n",
      "tensor([[-0.6979,  0.7364, -0.3177, -0.4024, -0.1583]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "53 correct_num: tensor(46, device='cuda:0')\n",
      "tensor([[ 0.7643, -0.0304, -0.0436, -0.4866,  0.1989]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "54 correct_num: tensor(47, device='cuda:0')\n",
      "tensor([[-0.3034, -0.8916, -0.0420,  0.9557, -0.4714]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "55 correct_num: tensor(48, device='cuda:0')\n",
      "tensor([[-0.8169, -0.3800, -0.7999,  0.3809, -0.9729]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "56 correct_num: tensor(49, device='cuda:0')\n",
      "tensor([[-0.8442,  0.5619, -0.4730, -0.2567, -0.3872]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "57 correct_num: tensor(50, device='cuda:0')\n",
      "tensor([[-0.4262, -0.8385, -0.1293,  0.9415, -0.5446]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "58 correct_num: tensor(51, device='cuda:0')\n",
      "tensor([[ 0.1784, -0.8619,  0.4246,  0.9137,  0.0217]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "59 correct_num: tensor(52, device='cuda:0')\n",
      "tensor([[-0.4290, -0.1590,  0.4254,  0.6537,  0.0925]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "60 correct_num: tensor(53, device='cuda:0')\n",
      "tensor([[ 0.8904, -0.5040,  0.5395,  0.1918,  0.4662]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "61 correct_num: tensor(54, device='cuda:0')\n",
      "tensor([[ 0.2375, -0.8906,  0.4221,  0.8980,  0.0381]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "62 correct_num: tensor(55, device='cuda:0')\n",
      "tensor([[ 0.0810,  0.9763,  0.0176, -0.9047,  0.4141]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([4], device='cuda:0')\n",
      "63 correct_num: tensor(55, device='cuda:0')\n",
      "tensor([[-0.5542, -0.5158,  0.1249,  0.8818, -0.2908]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "64 correct_num: tensor(56, device='cuda:0')\n",
      "tensor([[-0.7329, -0.4375, -0.1046,  0.7796, -0.4773]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "65 correct_num: tensor(57, device='cuda:0')\n",
      "tensor([[ 0.5734,  0.7050,  0.1267, -0.9121,  0.5351]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "66 correct_num: tensor(57, device='cuda:0')\n",
      "tensor([[-0.7432, -0.6047, -0.5088,  0.7090, -0.8184]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "67 correct_num: tensor(58, device='cuda:0')\n",
      "tensor([[-0.9593, -0.1279, -0.7427,  0.2836, -0.8789]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "68 correct_num: tensor(59, device='cuda:0')\n",
      "tensor([[ 0.3583, -0.9349,  0.2703,  0.7435, -0.0435]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "69 correct_num: tensor(59, device='cuda:0')\n",
      "tensor([[ 0.4051,  0.8008,  0.0070, -0.9712,  0.4380]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "70 correct_num: tensor(59, device='cuda:0')\n",
      "tensor([[ 0.9752, -0.0449,  0.4625, -0.3179,  0.6147]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "71 correct_num: tensor(60, device='cuda:0')\n",
      "tensor([[-0.3720, -0.8219,  0.0239,  0.9765, -0.4133]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "72 correct_num: tensor(61, device='cuda:0')\n",
      "tensor([[-0.9828,  0.0865, -0.7091,  0.1265, -0.7919]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "73 correct_num: tensor(62, device='cuda:0')\n",
      "tensor([[ 0.9361, -0.1690,  0.3032, -0.2699,  0.4444]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "74 correct_num: tensor(63, device='cuda:0')\n",
      "tensor([[ 0.7526, -0.4823,  0.0418, -0.0211,  0.1003]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "75 correct_num: tensor(63, device='cuda:0')\n",
      "tensor([[0.2027, 0.3436, 0.8452, 0.0770, 0.8043]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([4], device='cuda:0')\n",
      "76 correct_num: tensor(63, device='cuda:0')\n",
      "tensor([[-0.9375, -0.0317, -0.3343,  0.4285, -0.5365]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "77 correct_num: tensor(63, device='cuda:0')\n",
      "tensor([[-0.2334, -0.7941,  0.1294,  0.9613, -0.2671]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "78 correct_num: tensor(64, device='cuda:0')\n",
      "tensor([[-0.7390, -0.6104, -0.4303,  0.7508, -0.7660]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "79 correct_num: tensor(65, device='cuda:0')\n",
      "tensor([[-0.8330, -0.2799, -0.1873,  0.6693, -0.4970]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "80 correct_num: tensor(66, device='cuda:0')\n",
      "tensor([[-0.4251,  0.8713, -0.2558, -0.6360,  0.0414]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([4], device='cuda:0')\n",
      "81 correct_num: tensor(66, device='cuda:0')\n",
      "tensor([[-0.3914,  0.9385, -0.1567, -0.6849,  0.1140]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([2], device='cuda:0')\n",
      "82 correct_num: tensor(66, device='cuda:0')\n",
      "tensor([[ 0.2228,  0.9496,  0.1178, -0.9043,  0.5160]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([4], device='cuda:0')\n",
      "83 correct_num: tensor(66, device='cuda:0')\n",
      "tensor([[ 0.2222,  0.8188,  0.4801, -0.5713,  0.7446]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([4], device='cuda:0')\n",
      "84 correct_num: tensor(66, device='cuda:0')\n",
      "tensor([[-0.9013,  0.3363, -0.2913,  0.0871, -0.3791]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "85 correct_num: tensor(66, device='cuda:0')\n",
      "tensor([[ 0.7219,  0.6160,  0.3527, -0.7969,  0.7080]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "86 correct_num: tensor(67, device='cuda:0')\n",
      "tensor([[ 0.5461, -0.6145,  0.7831,  0.6588,  0.4860]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([2], device='cuda:0')\n",
      "87 correct_num: tensor(68, device='cuda:0')\n",
      "tensor([[-0.7277,  0.6495, -0.1522, -0.2095, -0.0957]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "88 correct_num: tensor(69, device='cuda:0')\n",
      "tensor([[-0.0609,  0.8275,  0.4188, -0.4625,  0.5743]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([2], device='cuda:0')\n",
      "89 correct_num: tensor(69, device='cuda:0')\n",
      "tensor([[ 0.9839, -0.0747,  0.4544, -0.2879,  0.6073]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "90 correct_num: tensor(70, device='cuda:0')\n",
      "tensor([[-0.3566, -0.8625, -0.0733,  0.9413, -0.5045]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([2], device='cuda:0')\n",
      "91 correct_num: tensor(70, device='cuda:0')\n",
      "tensor([[-0.9142, -0.3124, -0.4727,  0.5710, -0.7470]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "92 correct_num: tensor(71, device='cuda:0')\n",
      "tensor([[-0.3239,  0.8054, -0.5724, -0.8461, -0.1936]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "93 correct_num: tensor(72, device='cuda:0')\n",
      "tensor([[-0.5209,  0.8712, -0.1799, -0.5530,  0.0401]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "94 correct_num: tensor(73, device='cuda:0')\n",
      "tensor([[-0.6070,  0.8283, -0.3575, -0.5707, -0.1374]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "95 correct_num: tensor(74, device='cuda:0')\n",
      "tensor([[-0.3354, -0.8729, -0.0961,  0.9189, -0.5225]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "96 correct_num: tensor(75, device='cuda:0')\n",
      "tensor([[ 0.9906, -0.0900,  0.5258, -0.2428,  0.6465]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([0], device='cuda:0')\n",
      "97 correct_num: tensor(76, device='cuda:0')\n",
      "tensor([[ 0.0894, -0.9072,  0.3266,  0.9392, -0.0951]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "98 correct_num: tensor(77, device='cuda:0')\n",
      "tensor([[-0.7189, -0.6331, -0.5326,  0.6982, -0.8434]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "99 correct_num: tensor(78, device='cuda:0')\n",
      "tensor([[-0.4532,  0.8681, -0.0324, -0.5142,  0.1445]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "100 correct_num: tensor(79, device='cuda:0')\n",
      "tensor([[-0.4201,  0.8869, -0.4319, -0.7843, -0.1088]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([1], device='cuda:0')\n",
      "101 correct_num: tensor(80, device='cuda:0')\n",
      "tensor([[-0.5923,  0.2306,  0.2194,  0.2710,  0.0345]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "102 correct_num: tensor(81, device='cuda:0')\n",
      "tensor([[-0.3884,  0.8103,  0.1372, -0.4114,  0.2665]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "103 correct_num: tensor(81, device='cuda:0')\n",
      "tensor([[-0.4365, -0.7296,  0.0667,  0.9561, -0.3781]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "104 correct_num: tensor(82, device='cuda:0')\n",
      "tensor([[-0.9105, -0.2590, -0.3836,  0.5686, -0.6640]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "105 correct_num: tensor(83, device='cuda:0')\n",
      "tensor([[-0.8184, -0.4922, -0.4202,  0.6976, -0.7458]], device='cuda:0',\n",
      "       grad_fn=<MmBackward0>) True label: tensor([3], device='cuda:0')\n",
      "106 correct_num: tensor(84, device='cuda:0')\n",
      "Test_accuracy: tensor(0.7850, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class_ind = {'CC':0, 'EC':1, 'LGSC':2, 'HGSC':3, 'MC':4}\n",
    "\n",
    "def test_loop(dataloader, model, image_processor=None):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    texts = ['This type of cells have cell cytoplasm that are see through, and often have clear cell boundaries', \n",
    "             'Cells exhibit a back-to-back glandular pattern', \n",
    "             'This type of cells have cells close to normal healthy cells, cells containing single nuclei, and alive cell', \n",
    "             'There are many cells that are often deformed in shape, and many cells with multiple nucleus, and tissues often present many dead cells',\n",
    "             'This type of cells often have goblet cells, they are often goblet-like or cell-like']\n",
    "    texts = tokenizer(texts, padding='max_length', max_length=30, return_tensors='pt').to(device)\n",
    "    \n",
    "    correct_num = 0\n",
    "    for batch, (imgs, _, labels) in enumerate(dataloader):\n",
    "        imgs = imgs.view(-1, 3, 224, 224)\n",
    "        \n",
    "        if image_processor:\n",
    "            imgs = image_processor(imgs, return_tensors=\"pt\").to(device)\n",
    "            # Compute prediction and loss\n",
    "            logits = model(imgs['pixel_values'], texts)\n",
    "        else:\n",
    "            # Compute prediction and loss\n",
    "            logits = model(imgs, texts)\n",
    "        print(logits, 'True label:', labels)\n",
    "\n",
    "        correct_num += torch.sum(torch.argmax(logits, axis=1) == labels)\n",
    "        print(batch, 'correct_num:', correct_num)\n",
    "\n",
    "    test_acc = correct_num  / size\n",
    "    print('Test_accuracy:', test_acc)\n",
    "\n",
    "test_dataloader = DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "test_loop(test_dataloader, clip_model, image_processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples and our old Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Bert Example ############################################\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Define the path to your file\n",
    "file_path = '/scratch1/yuqiuwan/CSCI567/textLabel/CC.txt'\n",
    "output_CC = []\n",
    "\n",
    "# Open the file using the 'with' statement to ensure it gets closed after reading\n",
    "with open(file_path, 'r') as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Each 'line' includes a newline character at the end, you can strip it using strip()\n",
    "        clean_line = line.strip()\n",
    "        parts = clean_line.split('. ', 1)\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "        sample = parts[1]\n",
    "        encoding = tokenizer.encode(sample, max_length=512, padding=\"max_length\")\n",
    "        output_CC.append(encoding)\n",
    "\n",
    "output_CC = np.array(output_CC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = '/scratch1/yuqiuwan/CSCI567/preprocess_images/sample_10077/10077_0.png'\n",
    "img = np.asarray(Image.open(dir_path))\n",
    "img = img[13:237, 13:237]\n",
    "img = torch.tensor(img.T[None, :, :, :], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Phikon Example ############################################\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, ViTModel\n",
    "\n",
    "# load an image\n",
    "image = np.stack([img[0].reshape(250,250)]*3, axis=-1) # Image.open(\"assets/example.tif\")\n",
    "\n",
    "# load phikon\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"owkin/phikon\")\n",
    "model = ViTModel.from_pretrained(\"owkin/phikon\", add_pooling_layer=False)\n",
    "\n",
    "# process the image\n",
    "inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# get the features\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    features = outputs.last_hidden_state[:, 0, :] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Lunit Example ############################################\n",
    "import torch\n",
    "from timm.models.vision_transformer import VisionTransformer\n",
    "\n",
    "def get_pretrained_url(key):\n",
    "    URL_PREFIX = \"https://github.com/lunit-io/benchmark-ssl-pathology/releases/download/pretrained-weights\"\n",
    "    model_zoo_registry = {\n",
    "        \"DINO_p16\": \"dino_vit_small_patch16_ep200.torch\",\n",
    "        \"DINO_p8\": \"dino_vit_small_patch8_ep200.torch\",\n",
    "    }\n",
    "    pretrained_url = f\"{URL_PREFIX}/{model_zoo_registry.get(key)}\"\n",
    "    return pretrained_url\n",
    "\n",
    "\n",
    "def vit_small(pretrained, progress, key, **kwargs):\n",
    "    patch_size = kwargs.get(\"patch_size\", 16)\n",
    "    model = VisionTransformer(\n",
    "        img_size=224, patch_size=patch_size, embed_dim=384, num_heads=6, num_classes=0\n",
    "    )\n",
    "    if pretrained:\n",
    "        pretrained_url = get_pretrained_url(key)\n",
    "        verbose = model.load_state_dict(\n",
    "            torch.hub.load_state_dict_from_url(pretrained_url, progress=progress)\n",
    "        )\n",
    "        print(verbose)\n",
    "    return model\n",
    "\n",
    "model = vit_small(pretrained=True, progress=False, key=\"DINO_p16\", patch_size=16)\n",
    "t = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions \n",
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, d_embed, head_size, block_size=None):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(d_embed, head_size, bias=False)\n",
    "        self.block_size = block_size\n",
    "        self.head_size = head_size\n",
    "        if block_size != None:\n",
    "            self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Input: x of shape (B, T, C), i.e. (batch size, sequence length, input channels)\n",
    "          Output: tensor of shape (B, T, C)\n",
    "\n",
    "          (1) Computes key and query representations using linear transformations.\n",
    "          (2) Computes attention scores by multiplying query and key tensors and normalizing by C**-0.5.\n",
    "              Masks out future information using the lower triangular matrix (you can use tril function).\n",
    "              Applies softmax to get attention weights. Then, applies dropout for regularization\n",
    "          (3) Computes the weighted sum of values using attention weights.\n",
    "        \"\"\"\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        if self.block_size != None:\n",
    "            weight = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "            weight = weight.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        else:\n",
    "            weight = q @ k.transpose(-2,-1) * (self.head_size**-0.5)\n",
    "        weight = F.softmax(weight, dim=-1) # (B, T, T)\n",
    "        weight = self.dropout(weight)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = weight @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, d_embed, block_size=None):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(d_embed, head_size, block_size=block_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(d_embed, d_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Input: x of shape (B, T, C), i.e. (batch size, sequence length, input channels)\n",
    "          Output: tensor of shape (B, T, C)\n",
    "\n",
    "          (1) Computes attention for each head in parallel and concatenates their outputs along the last dimension.\n",
    "          (2) Projects concatenated head outputs back to the original dimension using a linear layer.\n",
    "          (3) Applies dropout for regularization.\n",
    "        \"\"\"\n",
    "    \n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, d_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_embed, 4 * d_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d_embed, d_embed),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        \"\"\" Input: x of shape (B, T, C), i.e. (batch size, sequence length, input channels)\n",
    "          Output: tensor of shape (B, T, C)\n",
    "        \"\"\"\n",
    "\n",
    "        return self.net(x)\n",
    "      \n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, d_embed, n_heads, block_size=None):\n",
    "        # d_embed: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        assert d_embed % n_heads == 0, f\"Can't divide dimension {d_embed} into {n_heads} heads\"\n",
    "        head_size = d_embed // n_heads\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size, d_embed, block_size=block_size)\n",
    "        self.ffwd = FeedFoward(d_embed)\n",
    "        self.ln1 = nn.LayerNorm(d_embed)\n",
    "        self.ln2 = nn.LayerNorm(d_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "    \n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, data_shape, n_patches=10, n_blocks=2, d_embed=10, n_heads=2, n_classes=5):\n",
    "        # Super constructor\n",
    "        super().__init__()\n",
    "        \n",
    "        # Attributes\n",
    "        self.chw = data_shape # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.n_blocks = n_blocks\n",
    "        self.n_heads = n_heads\n",
    "        self.d_embed = d_embed\n",
    "        \n",
    "        # Input and patches sizes\n",
    "        assert data_shape[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        assert data_shape[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (data_shape[1] / n_patches, data_shape[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(data_shape[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, d_embed)\n",
    "        \n",
    "        # 2) Learnable classification token\n",
    "        self.classify_token = nn.Parameter(torch.rand(1, 1, d_embed))\n",
    "        \n",
    "        # 3) Positional embedding\n",
    "        # self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, d_embed), persistent=False)\n",
    "        self.position_embedding_table = nn.Embedding(n_patches ** 2 + 1, d_embed)\n",
    "        \n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList([Block(d_embed, n_heads, block_size=None) for _ in range(n_blocks)])\n",
    "        \n",
    "        # 5) Classification MLPk\n",
    "        self.proj = nn.Linear(d_embed, n_classes)\n",
    "    \n",
    "\n",
    "    def forward(self, images):\n",
    "        # Dividing images into patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(device)\n",
    "        \n",
    "        # Running linear layer tokenization\n",
    "        # Map the vector corresponding to each patch to the hidden size dimension\n",
    "        tokens = self.linear_mapper(patches)\n",
    "        \n",
    "        # Adding classification token to the tokens\n",
    "        tokens = torch.cat((self.classify_token.expand(n, -1, -1), tokens), dim=1)\n",
    "        \n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.position_embedding_table(torch.arange(self.n_patches ** 2 + 1, device=device)) # self.positional_embeddings.repeat(n, 1, 1)\n",
    "        \n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "            \n",
    "        # Getting the Classify Token, which is a token that represents the entire input \n",
    "        out = out[:, 0]\n",
    "        logits = self.proj(out)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "class TextEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, block_size, n_blocks=2, d_embed=10, n_heads=2, n_classes=5):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, d_embed)\n",
    "        self.position_embedding_table = nn.Embedding(seq_max_length + 1, d_embed)\n",
    "        self.classify_token = nn.Parameter(torch.rand(1, 1, d_embed))\n",
    "\n",
    "        self.blocks = nn.Sequential(*[Block(d_embed, n_heads, block_size=block_size) for _ in range(n_blocks)])\n",
    "        self.ln_f = nn.LayerNorm(d_embed) # final layer norm\n",
    "        self.lm_head = nn.Linear(d_embed, n_classes)\n",
    "\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        tok_emb = torch.cat((self.classify_token.expand(B, -1, -1), tok_emb), dim=1)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T+1, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        x = x[:, 0] # get classify token\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
